{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import open\n",
    "import os\n",
    "from conllu import parse\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file = \"Trains_93.CONLL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1dc10b58ffc7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m                               fields=['id', 'form', 'lemma', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps',\n\u001b[0;32m     11\u001b[0m                                       'misc', 'identity', 'bridging', 'discourse_deixis', 'reference', 'nom_sem'],\n\u001b[1;32m---> 12\u001b[1;33m                               field_parsers={\"identity\": split_func, \"nom_sem\": split_func})\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\conllu\\__init__.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(data, fields, field_parsers, metadata_parsers)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mfields\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfields\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mfield_parsers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfield_parsers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mmetadata_parsers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetadata_parsers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     ))\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\conllu\\__init__.py\u001b[0m in \u001b[0;36mparse_incr\u001b[1;34m(in_file, fields, field_parsers, metadata_parsers)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mfields\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse_conllu_plus_fields\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetadata_parsers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetadata_parsers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparse_sentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         yield parse_token_and_metadata(\n\u001b[0;32m     33\u001b[0m             \u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\conllu\\parser.py\u001b[0m in \u001b[0;36mparse_sentences\u001b[1;34m(in_file)\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[0mbuf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mbuf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = \"\"\"\"\"\"\n",
    "max_count = 0\n",
    "with open(file, 'r', encoding='utf-8') as read_file:\n",
    "    for line in read_file:\n",
    "        if line != \"\\n\":\n",
    "            data += line\n",
    "        elif line == \"\\n\":\n",
    "            split_func = lambda line, i: line[i].split(\"|\")\n",
    "            sentences = parse(data,\n",
    "                              fields=['id', 'form', 'lemma', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps',\n",
    "                                      'misc', 'identity', 'bridging', 'discourse_deixis', 'reference', 'nom_sem'],\n",
    "                              field_parsers={\"identity\": split_func, \"nom_sem\": split_func})\n",
    "\n",
    "            sentence = sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['form','identity']]\n",
    "df.index = df.index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token_df = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### index, identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_li = token_df.identity.tolist()\n",
    "\n",
    "l_count = 0\n",
    "r_count = 0\n",
    "\n",
    "l_step_count = 0\n",
    "r_step_count = 0\n",
    "\n",
    "token_index = []\n",
    "identities = []\n",
    "temp_id = []\n",
    "l_index = []\n",
    "\n",
    "for index, identity in enumerate(identity_li):\n",
    "    l_count += ''.join(identity).count('(')\n",
    "    r_count += ''.join(identity).count(')')\n",
    "    l_step_count = ''.join(identity).count('(')\n",
    "    r_step_count += ''.join(identity).count(')')\n",
    "\n",
    "    ### index processing###\n",
    "    for i in range(l_step_count):\n",
    "        l_index.append(index + 1)\n",
    "\n",
    "    while (r_step_count):\n",
    "        if l_index:\n",
    "            l_idx = l_index.pop()\n",
    "            token_index.append((l_idx, index + 1))\n",
    "            r_step_count -= 1\n",
    "\n",
    "    if l_count == r_count and identity == ['_']:\n",
    "        l_count = 0\n",
    "        r_count = 0\n",
    "        continue\n",
    "\n",
    "    temp_id += identity\n",
    "\n",
    "    if l_count == r_count:  \n",
    "        if l_count == 1:  # 중복 X\n",
    "            identities.append(''.join(temp_id))\n",
    "            temp_id = []\n",
    "\n",
    "        else:  # 중복\n",
    "            redundant_id = ''.join(temp_id)\n",
    "            for i in range(l_count):\n",
    "                temp_redundant_id = redundant_id[redundant_id.rindex('('):redundant_id.index(')') + 1]  # entity 안의 entity\n",
    "                identities.append(temp_redundant_id)\n",
    "                \n",
    "                redundant_id = redundant_id.replace(temp_redundant_id, '')\n",
    "\n",
    "            temp_id = []\n",
    "            \n",
    "        l_count = 0\n",
    "        r_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(identities)\n",
    "# len(token_index)\n",
    "# token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(columns=['entity', 'index', 'identity', 'head'])\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['index'] = token_index\n",
    "new_df['identity'] = identities\n",
    "new_df.index = new_df.index+1\n",
    "new_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['entity'] = ['0']*len(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for index in token_index:\n",
    "    temp_token=[]\n",
    "    for idx in range(index[0]-1,index[-1]):\n",
    "        temp_token.append(sentences[0][idx]['form'])\n",
    "    tokens.append(' '.join(temp_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['entity'] = tokens\n",
    "new_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(identities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_identity = []\n",
    "pattern = re.compile(r\"Min=\\d+[,]{0,1}\\d*\")\n",
    "for identity in identities:\n",
    "    if identity == '':\n",
    "        head_identity.append('')\n",
    "    else:\n",
    "        head= re.findall(pattern, identity)[0]\n",
    "        head_id = head.replace(\"Min=\",\"\")\n",
    "        head_identity.append(head_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(head_identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['head'] = head_identity\n",
    "new_df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = new_df.copy()\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = result_df[['entity','index','head']]\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_change_idx = file.index('.')\n",
    "name_change = file[name_change_idx:]\n",
    "name_change\n",
    "file_json = file.replace(name_change,'_json')\n",
    "file_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result_df.to_json(orient='columns')\n",
    "parsed = json.loads(result)\n",
    "\n",
    "with open(file_json, 'w') as outfile:\n",
    "    json.dump(parsed, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
